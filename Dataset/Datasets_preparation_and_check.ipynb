{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the orignial movielens dataset into matrix (.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movielens1M dataset shape: (6040, 3706)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#column headers for the dataset\n",
    "## format according to Movielens: http://files.grouplens.org/datasets/movielens/ml-1m-README.txt\n",
    "data_cols = ['UserID','MovieID','Rating','Timestamp']\n",
    "item_cols = ['MovieID','Title','Genres']\n",
    "user_cols = ['UserID','Gender','Age','Occupation', 'zip code']\n",
    "\n",
    "#importing the data files onto dataframes\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::',names=user_cols, encoding='latin-1', engine='python')\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::',names=item_cols, encoding='latin-1', engine='python')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::',names=data_cols, encoding='latin-1', engine='python')\n",
    "\n",
    "dataset = pd.merge(pd.merge(movies, ratings),users)\n",
    "#print(dataset.head())\n",
    "new_data = pd.concat([dataset.MovieID,dataset.UserID,dataset.Rating], axis=1)\n",
    "new_matrix = new_data.pivot(index='MovieID', columns='UserID', values='Rating')\n",
    "movielens_npy = new_matrix.values\n",
    "movielens_npy[np.isnan(movielens_npy)]=0\n",
    "movielens_npy = movielens_npy.T\n",
    "print(\"movielens1M dataset shape:\", movielens_npy.shape)\n",
    "#np.save(\"movielens1m.npy\", movielens_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verb dataset is preprocessed from another paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb dataset size is: (282, 1528)\n",
      "0.9734251420296313\n"
     ]
    }
   ],
   "source": [
    "verb_data = np.load(\"Verb/right_matrix.npy\")\n",
    "print(\"verb dataset size is:\", verb_data.shape)\n",
    "print(np.sum(verb_data==0)/(verb_data.shape[0] * verb_data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face dataset shape is : (32256, 64)\n"
     ]
    }
   ],
   "source": [
    "### download cropped images from http://vision.ucsd.edu/extyaleb/CroppedYaleBZip/CroppedYale.zip\n",
    "### pick subject 4, there are 64 images for him with shape (192, 168)\n",
    "face_data = np.load(\"face_id_4.npy\")\n",
    "print(\"face dataset shape is :\", face_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "#python3 -m ENMF.Experiments.preprocess_audiomnist_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
